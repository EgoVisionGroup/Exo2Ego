<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <br>
    <div class="logo" style="text-align: center;">
        <a href="index.html">
            <img src="./assets/images/logo.png" style="width: 100px;">
        </a>
    </div>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video Understanding">
    <title>Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video Understanding</title>
    <script>

    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h2 class="title is-1 publication-title">Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video Understanding</h2>
                        <h2 style="font-weight: bold; font-size: 30px; color: red;">CVPR 2025 Submission</h2>
                        <br>
                        <!-- <span class="author-block">
                            <a target="_blank"
                                href="https://scholar.google.com/citations?hl=zh-CN&user=HGPmq2YAAAAJ">Haoyu&#160;Zhang</a><sup>1
                                    2</sup>,
                            <a target="_blank"
                                href="https://scholar.google.com/citations?user=KO77A2oAAAAJ&hl=en">Meng&#160;Liu</a><sup>3&#9993</sup>,
                            <a target="_blank"
                                href="https://scholar.google.com/citations?user=9Vc--XsAAAAJ&hl=en&oi=ao">Yaowei&#160;Wang</a><sup>1
                                    2</sup>,
                            <br>
                            <a target="_blank"
                                href="https://scholar.google.com/citations?hl=en&user=yywVMhUAAAAJ">Liqiang&#160;Nie</a><sup>1&#9993</sup>
                            <div class="is-size-5 publication-authors" style="font-size: 10px;">
                                <span class="author-block"><sup>1</sup>Harbin Institute of Technology,
                                    Shenzhen&#160&#160&#160</span>
                                <span class="author-block"><sup>2</sup>Pengcheng Laboratory, Shenzhen</span>
                                <br>
                                <span class="author-block"><sup>3</sup>Shandong Jianzhu University, Jinan</span>
                            </div>


                            <div class="is-size-5 publication-authors" style="font-size: 10px;">
                                <span class="author-block"><sup>&#9993&#160;</sup>Corresponding
                                    author&#160;&#160;</span>
                            </div> -->
                            <div class="column has-text-centered">
                                <div class="publication-links">
                                    <!-- TODO PDF Link. -->
                                    <span class="link-block">
                                        <a target="_blank" href=""
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fas fa-file-pdf"></i>
                                            </span>
                                            <span>Paper (Coming)</span>
                                        </a>
                                    </span>
                                    <span class="link-block">
                                        <a target="_blank" href=""
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fas fa-database"></i>
                                            </span>
                                            <span>Data (Coming)</span>
                                        </a>
                                    </span>


                                    <!-- Code Link. -->
                                    <span class="link-block">
                                        <a target="_blank" href=""
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fab fa-github"></i>
                                            </span>
                                            <span>Code (Coming)</span>
                                        </a>
                                    </span>
                                    <br />

                                </div>

                            </div>
                    </div>

                </div>
            </div>
        </div>
    </section>

    <section class="section" style="background-color:#efeff081">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p style="font-size: 125%">
                            AI personal assistants deployed through robots or wearables need embodied understanding to collaborate effectively with humans. Existing Multimodal Large Language Models (MLLMs) focus on third-person (exocentric) vision, ignoring the uniqueness of first-person (egocentric) videos. And high acquisition costs limit data size, which in turn impairs the performance of MLLMs.
To address the above issues, we propose to learn the mapping between exocentric and egocentric data and utilize the rich exocentric knowledge embedded in existing MLLMs to guide the understanding of egocentric videos. Specifically, we create <b>Ego-ExoClip</b>, a synchronized first- and third-person video-text dataset comprising 1.1M ego-exo clip-text pairs well-chosen from Ego-Exo4D. 
Then, we design a progressive training pipeline, including three stages: <b>Teacher Self-Preparation, Teacher-Student Instruction, and Student Self-Practice.</b> Besides, we also propose instruction-tuning data to enhance the model's instruction-following capabilities, as well as a egocentric video understanding benchmark <b>EgoBench</b> for comprehensive evaluation.
Extensive experimental results on multiple egocentric tasks reveal that the existing MLLMs are far from satisfactory in egocentric video understanding, while our model largely surpasses these leading models.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section" style="background-color: #ffffff">
        <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3">Method</h2>
                        <img src="assets/images/model.jpg" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto" width="1000" height="900" />
                        <br>

                    </div>
                </div>
        </div>
    </section>

    <section class="section" style="background-color:#ffffff">
        <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="row is-full-width">
                        <h2 class="title is-3">Experiment</h2>
                        <img src="assets/images/leida.jpg" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto" width="500" height="500" />
                        <br>

                    </div>
                </div>
        </div>
    </section>

    <!--Conclusion-->
    <section class="section" style="background-color:#efeff081 ">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column">
                        <h2 class="title is-3"><span class="dvima">Conclusion</span></h2>
                        <div class="content has-text-justified">
                        <p style="font-size: 125%">
                            In this paper, we aim to advance egocentric video understanding by leveraging extensive exocentric knowledge integrated with MLLMs. To this end, we first construct a synchronized first- and third-person video-text dataset based on Ego-Exo4D. Next, we design a progressive three-stage training pipeline to learn the mappings between exocentric and egocentric domains. Additionally, to enhance instruction-following abilities for downstream tasks, we introduce egocentric instruction data. To comprehensively evaluate egocentric video understanding performance, we propose a new benchmark, EgoBench, derived from multiple sources. Experimental results have validated that our Exo2Ego framework outperforms existing MLLMs.
                        </p>

                        </div>

                </div>
            </div>
        </div>
    </section>


</body>

</html>